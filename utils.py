import streamlit as st
import sqlite3
import pandas as pd
import os
import shutil
import smtplib
import hashlib
import time
from datetime import datetime, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from email.mime.application import MIMEApplication

# å¼•å…¥ä½ çš„ä¸šåŠ¡é€»è¾‘æ¨¡å—
# æ³¨æ„ï¼šç¡®ä¿è¿™äº›æ–‡ä»¶åœ¨æ ¹ç›®å½•ä¸‹
import recalc_fund_history
from data_provider import DataProvider

# ==========================================
# 1. åŸºç¡€é…ç½®ä¸å¸¸é‡
# ==========================================

# ğŸ“ æ ‘è“æ´¾/ç¯å¢ƒåˆ¤æ–­é€»è¾‘
if os.path.exists('/share'):
    DB_FILE = '/share/asset_tracker.db'
    IS_RASPBERRY_PI = True
    # ç¡¬ç›˜ç¼“å­˜æ¨¡å¼
    CACHE_PARAMS = {
        "persist": "disk", 
        "ttl": None, 
        "show_spinner": "æ­£åœ¨ä»ç¡¬ç›˜è¯»å–å†å²æ•°æ® (æ ‘è“æ´¾æ¨¡å¼)..."
    }
else:
    DB_FILE = 'asset_tracker.db'
    IS_RASPBERRY_PI = False
    # å¼€å‘æ¨¡å¼ï¼šä¸ç¼“å­˜
    CACHE_PARAMS = {
        "persist": None, 
        "ttl": 0, 
        "show_spinner": "æ­£åœ¨å®æ—¶è®¡ç®— (PCå¼€å‘æ¨¡å¼)..."
    }

# ==========================================
# 2. æ•°æ®åº“åŸºç¡€æ“ä½œ
# ==========================================

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥ (Row Factory)"""
    conn = sqlite3.connect(DB_FILE)
    conn.row_factory = sqlite3.Row
    return conn

def init_db():
    """åˆå§‹åŒ–æ•°æ®åº“ (åŒ…å« V4 æ‰€æœ‰æœ€æ–°è¡¨ç»“æ„)"""
    if not os.path.exists(DB_FILE):
        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œæˆ–è€…ä½ å¸Œæœ›æ¯æ¬¡å¯åŠ¨æ£€æŸ¥è¡¨ç»“æ„ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ‰§è¡Œå»ºè¡¨ SQL
        # ä¸ºèŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œå‡è®¾ä½ å·²ç»è¿è¡Œäº† init_db.py
        # å¦‚æœéœ€è¦å®Œå…¨è‡ªåŠ¨åˆå§‹åŒ–ï¼Œå¯ä»¥å°†ç”Ÿæˆçš„ init_db.py å†…å®¹ç²˜è´´åˆ°è¿™é‡Œ
        pass

# ==========================================
# 3. ç”¨æˆ·ä¸ä¾§è¾¹æ é€»è¾‘ (æ ¸å¿ƒé‡æ„)
# ==========================================

def get_all_usernames():
    """è·å–æ‰€æœ‰ç”¨æˆ·ååˆ—è¡¨"""
    conn = get_db_connection()
    try:
        users = conn.execute('SELECT username FROM users').fetchall()
        return [u['username'] for u in users]
    except Exception:
        return []
    finally:
        conn.close()

def get_or_create_user_by_name(username):
    """è·å–æˆ–åˆ›å»ºç”¨æˆ·"""
    conn = get_db_connection()
    try:
        user = conn.execute('SELECT * FROM users WHERE username = ?', (username,)).fetchone()
        if user:
            return dict(user)
        else:
            dummy_hash = hashlib.sha256("123456".encode()).hexdigest() 
            cursor = conn.execute('INSERT INTO users (username, password_hash) VALUES (?, ?)', 
                                 (username, dummy_hash))
            user_id = cursor.lastrowid
            conn.commit()
            return {'user_id': user_id, 'username': username}
    except Exception as e:
        st.error(f"ç”¨æˆ·è·å–å¤±è´¥: {e}")
        return None
    finally:
        conn.close()

def delete_user_fully(target_user_id):
    """å½»åº•çº§è”åˆ é™¤ç”¨æˆ·"""
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        # 1. åˆ å…³è”
        assets = conn.execute('SELECT asset_id FROM assets WHERE user_id = ?', (target_user_id,)).fetchall()
        asset_ids = [str(row['asset_id']) for row in assets]
        
        if asset_ids:
            placeholders = ','.join(['?'] * len(asset_ids))
            cursor.execute(f'DELETE FROM snapshots WHERE asset_id IN ({placeholders})', asset_ids)
            cursor.execute(f'DELETE FROM asset_tag_map WHERE asset_id IN ({placeholders})', asset_ids)

        # 2. åˆ å„è¡¨æ•°æ®
        tables_with_userid = [
            'assets', 'tags', 'cashflows', 'investment_plans', 
            'investment_notes', 'monthly_profits', 'monthly_reviews', 
            'rebalance_targets', 'user_sessions', 'my_fund_history'
        ]
        for table in tables_with_userid:
            try:
                cursor.execute(f'DELETE FROM {table} WHERE user_id = ?', (target_user_id,))
            except sqlite3.OperationalError:
                pass # é˜²æ­¢è¡¨ä¸å­˜åœ¨æŠ¥é”™

        # 3. åˆ ç”¨æˆ·
        cursor.execute('DELETE FROM users WHERE user_id = ?', (target_user_id,))
        conn.commit()
        return True, "åˆ é™¤æˆåŠŸ"
    except Exception as e:
        conn.rollback()
        return False, f"åˆ é™¤å¤±è´¥: {str(e)}"
    finally:
        conn.close()

def show_sidebar_user_picker():
    """
    ğŸ“Œ å…¬å…±ä¾§è¾¹æ ç»„ä»¶ï¼šè´Ÿè´£ç”¨æˆ·åˆ‡æ¢ã€Demo æç¤ºã€æ ‘è“æ´¾åˆ·æ–°
    åœ¨æ¯ä¸ª Page æ–‡ä»¶çš„å¼€å¤´è°ƒç”¨æ­¤å‡½æ•°
    """
    with st.sidebar:
        
        # 1. è·å–ç”¨æˆ·åˆ—è¡¨
        existing_users = get_all_usernames()
        menu_options = existing_users + ["â• æ–°å¢æˆå‘˜..."]
        
        # 2. ç¡®å®šé»˜è®¤é€‰ä¸­é¡¹
        default_index = 0
        if 'user' in st.session_state and st.session_state.user:
            current_name = st.session_state.user['username']
            if current_name in existing_users:
                default_index = existing_users.index(current_name)
        
        # 3. ä¸‹æ‹‰æ¡†
        selected_option = st.selectbox(
            "å½“å‰æˆå‘˜", 
            menu_options, 
            index=default_index,
            key="user_selector_global"
        )

        # 4. é€»è¾‘å¤„ç†
        if selected_option == "â• æ–°å¢æˆå‘˜...":
            st.info("ğŸ‘‹ æ¬¢è¿æ–°æˆå‘˜åŠ å…¥ï¼")
            new_username = st.text_input("è¯·è¾“å…¥æ˜µç§°", placeholder="ä¾‹å¦‚ï¼šå¥¶å¥¶")
            if st.button("ç¡®è®¤åˆ›å»º"):
                if new_username.strip() and new_username not in existing_users:
                    new_user = get_or_create_user_by_name(new_username)
                    st.session_state.user = new_user
                    st.success(f"æ¬¢è¿ {new_username}ï¼")
                    st.rerun()
                elif new_username in existing_users:
                    st.error("åå­—å·²å­˜åœ¨")
            st.stop() # æš‚åœåç»­é¡µé¢æ¸²æŸ“
            
        else:
            # åˆ‡æ¢ç”¨æˆ· Session
            if 'user' not in st.session_state or st.session_state.user is None or st.session_state.user['username'] != selected_option:
                user_obj = get_or_create_user_by_name(selected_option)
                st.session_state.user = user_obj
                st.toast(f"å·²åˆ‡æ¢åˆ°è´¦æˆ·: {selected_option}", icon="ğŸ‘‹")
                st.rerun()

        st.divider()
        
        # Demo æç¤º
        if 'user' in st.session_state and st.session_state.user and st.session_state.user['username'] == 'demo':
            st.warning("âš ï¸ æ¼”ç¤ºæ¨¡å¼", icon="ğŸ¤–")

        # æ ‘è“æ´¾å¼ºåˆ¶åˆ·æ–°
        if IS_RASPBERRY_PI:
            if st.button("ğŸ”„ å¼ºåˆ¶åˆ·æ–°ç¼“å­˜"):
                st.cache_data.clear()
                st.toast("ç¼“å­˜å·²æ¸…é™¤", icon="ğŸš€")
                st.rerun()
                
        # æ˜¾ç¤ºå½“å‰ç”¨æˆ·ä¿¡æ¯
        if 'user' in st.session_state and st.session_state.user:
             st.caption(f"å½“å‰ç”¨æˆ· ID: {st.session_state.user['user_id']}")

# ==========================================
# 4. æ•°æ®ç¼–è¾‘ä¸åŒæ­¥ (Data Editor Utils)
# ==========================================

def save_changes_to_db(edited_df, original_df, table_name, id_col, user_id, fixed_cols=None):
    """å¤„ç† DataEditor çš„å¢åˆ æ”¹"""
    conn = get_db_connection()
    cursor = conn.cursor()
    try:
        # 1. åˆ é™¤å¤„ç†
        if not original_df.empty and not edited_df.empty:
            orig_ids = set(original_df[id_col].dropna().astype(int))
            new_ids = set(edited_df[id_col].dropna().astype(int))
            deleted_ids = orig_ids - new_ids
        elif not original_df.empty and edited_df.empty:
            deleted_ids = set(original_df[id_col].dropna().astype(int))
        else:
            deleted_ids = set()

        for del_id in deleted_ids:
            # çº§è”åˆ é™¤èµ„äº§ç›¸å…³å­è¡¨
            if table_name == 'assets':
                cursor.execute('DELETE FROM snapshots WHERE asset_id = ?', (del_id,))
                cursor.execute('DELETE FROM asset_tag_map WHERE asset_id = ?', (del_id,))
            elif table_name == 'tags':
                cursor.execute('DELETE FROM asset_tag_map WHERE tag_id = ?', (del_id,))
            
            cursor.execute(f'DELETE FROM {table_name} WHERE {id_col} = ? AND user_id = ?', (del_id, user_id))

        # 2. æ–°å¢ä¸ä¿®æ”¹
        for index, row in edited_df.iterrows():
            data = row.to_dict()
            if fixed_cols: data.update(fixed_cols)
            
            # æ–°å¢ (IDä¸ºç©ºæˆ–0)
            if pd.isna(row[id_col]) or row[id_col] == 0:
                cols = [k for k in data.keys() if k != id_col and k != 'created_at']
                placeholders = ', '.join(['?'] * len(cols))
                col_names = ', '.join(cols)
                values = [data[c] for c in cols]
                cursor.execute(f"INSERT INTO {table_name} ({col_names}) VALUES ({placeholders})", values)
            
            # ä¿®æ”¹
            elif row[id_col] in (original_df[id_col].values if not original_df.empty else []):
                cols = [k for k in data.keys() if k != id_col and k != 'created_at']
                set_clause = ', '.join([f"{c} = ?" for c in cols])
                values = [data[c] for c in cols]
                values.append(row[id_col])
                values.append(user_id)
                cursor.execute(f"UPDATE {table_name} SET {set_clause} WHERE {id_col} = ? AND user_id = ?", values)

        conn.commit()
        st.success("æ•°æ®åŒæ­¥æˆåŠŸï¼")
        return True
    except Exception as e:
        conn.rollback()
        st.error(f"ä¿å­˜å¤±è´¥: {str(e)}")
        return False
    finally:
        conn.close()

# ==========================================
# 5. æ ¸å¿ƒè®¡ç®—ä¸ä¸šåŠ¡é€»è¾‘
# ==========================================

def execute_daily_plans_safe(user_id, target_date_str):
    """æ‰§è¡Œå®šæŠ•è®¡åˆ’"""
    conn = get_db_connection()
    logs = []
    try:
        plans = conn.execute('''
            SELECT p.*, a.code, a.type, a.name as asset_name
            FROM investment_plans p
            JOIN assets a ON p.asset_id = a.asset_id
            WHERE p.user_id = ? AND p.is_active = 1
        ''', (user_id,)).fetchall()
        
        target_date = datetime.strptime(target_date_str, '%Y-%m-%d').date()
        
        for plan in plans:
            # 1. æ—¥æœŸæ£€æŸ¥
            freq = plan['frequency']
            exec_day = int(plan['execution_day'])
            is_due = False
            if freq == 'æ¯å¤©': is_due = True
            elif freq == 'æ¯å‘¨' and target_date.weekday() == exec_day: is_due = True
            elif freq == 'æ¯æœˆ' and target_date.day == exec_day: is_due = True
            
            if not is_due: continue

            # 2. å¹‚ç­‰æ£€æŸ¥
            check_note_pattern = f"è‡ªåŠ¨å®šæŠ•: {plan['asset_name']}"
            exist_flow = conn.execute('''
                SELECT id FROM cashflows 
                WHERE user_id = ? AND date = ? AND category = 'å®šæŠ•æ‰£æ¬¾' AND note LIKE ?
            ''', (user_id, target_date_str, f"%{check_note_pattern}%")).fetchone()
            if exist_flow:
                logs.append(f"â­ï¸ è·³è¿‡: {plan['asset_name']} (ä»Šæ—¥å·²æ‰§è¡Œ)")
                continue

            # 3. æ‰§è¡Œé€»è¾‘
            amount = plan['amount']
            asset_id = plan['asset_id']
            code = plan['code']
            
            nav = 1.0
            if plan['type'] in ['åŸºé‡‘', 'è‚¡ç¥¨'] and code:
                nav = DataProvider.get_fund_nav(code) or 1.0
            
            shares_to_add = amount / nav
            
            # æ›´æ–°æˆæœ¬ (ç§»åŠ¨åŠ æƒ)
            curr = conn.execute('SELECT last_shares, unit_cost FROM assets WHERE asset_id=?', (asset_id,)).fetchone()
            old_shares = curr['last_shares'] or 0.0
            old_cost = curr['unit_cost'] or 0.0
            new_shares = old_shares + shares_to_add
            new_unit_cost = ((old_shares * old_cost) + amount) / new_shares if new_shares > 0 else 0
            
            conn.execute('UPDATE assets SET last_shares=?, unit_cost=? WHERE asset_id=?', (new_shares, new_unit_cost, asset_id))
            
            # æ‰£å‡æ¥æº
            if plan['source_asset_id']:
                src = conn.execute('SELECT last_shares FROM assets WHERE asset_id=?', (plan['source_asset_id'],)).fetchone()
                src_shares = (src['last_shares'] or 0.0) - amount
                conn.execute('UPDATE assets SET last_shares=? WHERE asset_id=?', (src_shares, plan['source_asset_id']))
                # è®°å½•å¿«ç…§
                conn.execute('''
                    INSERT INTO snapshots (asset_id, date, amount, profit, cost, shares, unit_nav, is_cleared)
                    VALUES (?, ?, ?, 0, ?, ?, 1.0, 0)
                    ON CONFLICT(asset_id, date) DO UPDATE SET amount=excluded.amount, shares=excluded.shares
                ''', (plan['source_asset_id'], target_date_str, src_shares, src_shares, src_shares))

            # è®°æµæ°´
            note = f"{check_note_pattern} (ä»½é¢+{shares_to_add:.2f}, æ–°æˆæœ¬{new_unit_cost:.4f})"
            conn.execute("INSERT INTO cashflows (user_id, date, type, amount, category, note, created_at) VALUES (?, ?, 'æ”¯å‡º', ?, 'å®šæŠ•æ‰£æ¬¾', ?, datetime('now'))",
                         (user_id, target_date_str, amount, note))
            logs.append(f"âœ… æ‰§è¡Œ: {plan['asset_name']} {amount}å…ƒ")

        conn.commit()
        return True, logs
    except Exception as e:
        return False, [f"é”™è¯¯: {e}"]
    finally:
        conn.close()

def recalculate_daily_nav(user_id, target_date_str, progress_bar=None, status_text=None, limit_asset_ids=None):
    """ä¸€é”®æ›´æ–°è¡Œæƒ…å‡€å€¼"""
    conn = get_db_connection()
    results = {"success": [], "fail": []}
    try:
        sql = 'SELECT asset_id, name, code, type, last_shares, unit_cost FROM assets WHERE user_id = ? AND auto_update = 1 AND code IS NOT NULL'
        params = [user_id]
        if limit_asset_ids:
            placeholders = ','.join(['?'] * len(limit_asset_ids))
            sql += f" AND asset_id IN ({placeholders})"
            params.extend(limit_asset_ids)
        else:
            return True, {"success": [], "fail": ["æœªé€‰ä¸­èµ„äº§"]}

        targets = conn.execute(sql, params).fetchall()
        total = len(targets)
        if total == 0: return True, {"success":[], "fail":["æ— è‡ªåŠ¨æ›´æ–°èµ„äº§"]}

        for idx, asset in enumerate(targets):
            if progress_bar: progress_bar.progress(idx / total)
            if status_text: status_text.caption(f"æ›´æ–°: {asset['name']}...")
            
            try:
                nav = 1.0
                if 'åŸºé‡‘' in asset['type']:
                    nav = DataProvider.get_fund_nav(asset['code'], end_date=target_date_str)
                elif 'è‚¡ç¥¨' in asset['type']:
                    nav = DataProvider.get_stock_price(asset['code'])
                
                # è·å–è¯¥æ—¥å¿«ç…§æˆ–æœ€æ–°ä»½é¢
                snap = conn.execute('SELECT shares FROM snapshots WHERE asset_id=? AND date=?', (asset['asset_id'], target_date_str)).fetchone()
                shares = snap['shares'] if (snap and snap['shares'] > 0) else asset['last_shares']
                
                cost = asset['unit_cost'] or 0.0
                amt = shares * nav
                profit = amt - (shares * cost)
                
                conn.execute('''
                    INSERT INTO snapshots (asset_id, date, amount, profit, cost, shares, unit_nav, is_cleared)
                    VALUES (?, ?, ?, ?, ?, ?, ?, 0)
                    ON CONFLICT(asset_id, date) DO UPDATE SET
                    amount=excluded.amount, profit=excluded.profit, unit_nav=excluded.unit_nav
                ''', (asset['asset_id'], target_date_str, amt, profit, shares * cost, shares, nav))
                
                results['success'].append(f"{asset['name']}: {nav}")
            except Exception as e:
                results['fail'].append(f"{asset['name']}: {e}")

        conn.commit()
        if progress_bar: progress_bar.progress(1.0)
        return True, results
    finally:
        conn.close()

def get_latest_rates(conn):
    """è·å–æœ€æ–°æ±‡ç‡å­—å…¸"""
    df = pd.read_sql("SELECT currency, rate, date FROM exchange_rates ORDER BY date DESC", conn)
    if df.empty: return {}
    return df.drop_duplicates(subset=['currency']).set_index('currency')['rate'].to_dict()

# ==========================================
# 6. åˆ†æä¸æŠ¥è¡¨ (Analytics)
# ==========================================

@st.cache_data(**CACHE_PARAMS)
def get_cached_analytics_data(user_id):
    """è·å–å¸¦ç¼“å­˜çš„èµ„äº§åˆ†ææ•°æ®"""
    local_conn = sqlite3.connect(DB_FILE)
    try:
        # 1. åŸºç¡€å¿«ç…§
        df_raw = pd.read_sql('''
            SELECT s.date, s.asset_id, s.amount, s.profit, s.cost, s.yield_rate, a.name, a.currency, a.type
            FROM snapshots s JOIN assets a ON s.asset_id = a.asset_id
            WHERE a.user_id = ?
        ''', local_conn, params=(user_id,))
        if df_raw.empty: return None, None
        df_raw['date'] = pd.to_datetime(df_raw['date'])

        # 2. æ±‡ç‡æ¢ç®—
        df_rates = pd.read_sql("SELECT date, currency, rate FROM exchange_rates", local_conn)
        df_rates['date'] = pd.to_datetime(df_rates['date'])
        df_merged = pd.merge(df_raw, df_rates, on=['date', 'currency'], how='left')
        df_merged['rate'] = df_merged.apply(lambda r: 1.0 if r['currency'] == 'CNY' else r['rate'], axis=1).fillna(1.0)
        
        for col in ['amount', 'profit', 'cost']:
            df_merged[col] = df_merged[col] * df_merged['rate']

        # 3. æ ‡ç­¾èšåˆ
        df_tags = pd.read_sql('SELECT t.tag_group, t.tag_name, atm.asset_id FROM tags t JOIN asset_tag_map atm ON t.tag_id=atm.tag_id WHERE t.user_id=?', local_conn, params=(user_id,))
        
        df_tags_agg = pd.DataFrame()
        if not df_tags.empty:
            merged = pd.merge(df_merged, df_tags, on='asset_id', how='inner')
            tag_analytics = []
            for name, group in merged.groupby(['date', 'tag_group', 'tag_name']):
                d, tg, tn = name
                tag_analytics.append({
                    'date': d, 'tag_group': tg, 'tag_name': tn,
                    'amount': group['amount'].sum(),
                    'profit': group['profit'].sum(),
                    'cost': group['cost'].sum(),
                    'yield_rate': (group['profit'].sum()/group['cost'].sum()*100) if group['cost'].sum()!=0 else 0
                })
            df_tags_agg = pd.DataFrame(tag_analytics)

        return df_merged, df_tags_agg
    finally:
        local_conn.close()

@st.cache_data(ttl=3600*12)
def get_market_index_data_cached(index_name, start_date_str, end_date_str):
    """ç¼“å­˜æŒ‡æ•°æ•°æ®"""
    return DataProvider.get_market_index_data(index_name, start_date_str, end_date_str)

# ==========================================
# 7. AI ä¸ å¤‡ä»½ç³»ç»Ÿ
# ==========================================

def generate_and_send_ai_prompt(user_id, start_date_str, end_date_str, dimension_group):
    """
    ç”Ÿæˆ AI é¡¾é—®æç¤ºè¯ (ä¸“ä¸šç‰ˆï¼šåŒ…å«æ¯æ—¥å‡€å€¼CSV + ç»“æ„å¯¹æ¯” + æ ¸å¿ƒæŒä»“)
    :param dimension_group: "æŒ‰å…·ä½“èµ„äº§" æˆ– å…·ä½“çš„æ ‡ç­¾ç»„åç§° (å¦‚ "èµ„äº§å¤§ç±»")
    """
    import pandas as pd
    import smtplib
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart
    
    conn = get_db_connection()
    
    # --- 1. è·å–ç³»ç»Ÿé‚®ç®±è®¾ç½® ---
    settings = conn.execute('SELECT * FROM system_settings WHERE id = 1').fetchone()
    if not settings['email_host']:
        conn.close()
        return False, "æœªé…ç½®é‚®ç®± SMTPï¼Œæ— æ³•å‘é€ã€‚"

    try:
        # --- 2. å‡†å¤‡æ¯æ—¥è¶‹åŠ¿æ•°æ® (Daily Trend) ---
        # ç›´æ¥ä» fund_history å–ï¼Œå› ä¸ºé‚£é‡Œæœ‰è®¡ç®—å¥½çš„å‡€å€¼ (NAV)
        sql_trend = '''
            SELECT date, total_assets, accumulated_profit, unit_nav 
            FROM my_fund_history 
            WHERE user_id = ? AND date BETWEEN ? AND ?
            ORDER BY date ASC
        '''
        df_trend = pd.read_sql(sql_trend, conn, params=(user_id, start_date_str, end_date_str))
        
        if df_trend.empty:
            return False, f"è¯¥æ—¶é—´æ®µ ({start_date_str} ~ {end_date_str}) å†…æ²¡æœ‰ç”Ÿæˆè¿‡å‡€å€¼å†å²æ•°æ®ï¼Œè¯·å…ˆç¡®ä¿å·²è¿›è¡Œè¿‡æ•°æ®å½•å…¥å’Œé‡ç®—ã€‚"
        
        # æ‰‹åŠ¨æ„å»º Markdown è¡¨æ ¼ (æ—¥æœŸ | æ€»èµ„äº§ | ç´¯è®¡æ”¶ç›Š | å•ä½å‡€å€¼)
        trend_lines = ["| æ—¥æœŸ | æ€»èµ„äº§ | ç´¯è®¡æ”¶ç›Š | å•ä½å‡€å€¼ |"]
        trend_lines.append("|---|---|---|---|")
        for _, r in df_trend.iterrows():
            trend_lines.append(f"| {r['date']} | {r['total_assets']:.2f} | {r['accumulated_profit']:.2f} | {r['unit_nav']:.4f} |")
        markdown_trend_str = "\n".join(trend_lines)
        
        # --- 3. å‡†å¤‡æœŸåˆ vs æœŸæœ« ç»“æ„å¯¹æ¯” (Structure Comparison) ---
        # æ ¹æ®ç”¨æˆ·é€‰æ‹©çš„ç»´åº¦ (dimension_group) è·å–æ•°æ®
        # æˆ‘ä»¬åˆ©ç”¨ get_cached_analytics_data è·å–å¿«ç…§æ•°æ®
        df_assets_all, df_tags_all = get_cached_analytics_data(user_id)
        
        # ç­›é€‰æ—¥æœŸ
        start_date = pd.to_datetime(start_date_str)
        end_date = pd.to_datetime(end_date_str)
        
        target_df = pd.DataFrame()
        group_col = ""
        
        if dimension_group == "æŒ‰å…·ä½“èµ„äº§":
            # ä½¿ç”¨ df_assets_all
            mask = df_assets_all['date'].isin([start_date, end_date])
            target_df = df_assets_all[mask].copy()
            group_col = "name"
        else:
            # ä½¿ç”¨ df_tags_all
            mask = (df_tags_all['date'].isin([start_date, end_date])) & (df_tags_all['tag_group'] == dimension_group)
            target_df = df_tags_all[mask].copy()
            group_col = "tag_name"

        structure_str = ""
        if target_df.empty:
            structure_str = "(è¯¥ç»´åº¦ä¸‹æš‚æ— æ•°æ®)"
        else:
            # é€è§†è¡¨ï¼šIndex=åç§°, Column=æ—¥æœŸ, Value=é‡‘é¢
            pivot = target_df.pivot_table(index=group_col, columns='date', values='amount', aggfunc='sum').fillna(0)
            
            # ç¡®ä¿åˆ—åå­˜åœ¨ï¼ˆé˜²æ­¢æŸä¸€æœŸå®Œå…¨æ²¡æ•°æ®ï¼‰
            if start_date not in pivot.columns: pivot[start_date] = 0.0
            if end_date not in pivot.columns: pivot[end_date] = 0.0
            
            # è®¡ç®—æ€»é¢ç”¨äºç®—å æ¯”
            total_start = pivot[start_date].sum()
            total_end = pivot[end_date].sum()
            
            # æ ¼å¼åŒ–è¾“å‡º
            lines = []
            # æŒ‰æœŸæœ«é‡‘é¢é™åºæ’
            pivot = pivot.sort_values(by=end_date, ascending=False)
            
            lines.append(f"| {group_col} | æœŸåˆé‡‘é¢ | æœŸåˆå æ¯” | æœŸæœ«é‡‘é¢ | æœŸæœ«å æ¯” | å˜åŠ¨é¢ |")
            lines.append(f"|---|---|---|---|---|---|")
            
            for name, row in pivot.iterrows():
                s_amt = row[start_date]
                e_amt = row[end_date]
                # å¿½ç•¥å¤ªå°çš„æ‚é¡¹ï¼Œå‡å°‘ token
                if s_amt < 100 and e_amt < 100: continue
                
                s_pct = (s_amt / total_start * 100) if total_start > 0 else 0
                e_pct = (e_amt / total_end * 100) if total_end > 0 else 0
                diff = e_amt - s_amt
                
                lines.append(f"| {name} | {s_amt:.0f} | {s_pct:.1f}% | {e_amt:.0f} | {e_pct:.1f}% | {diff:+.0f} |")
            
            structure_str = "\n".join(lines)

        # --- 4. æ ¸å¿ƒæŒä»“åˆ†æ (>0.5%) ---
        # ä»…é’ˆå¯¹ Period End Date
        top_holdings_str = ""
        mask_end = df_assets_all['date'] == end_date
        if not mask_end.any():
            top_holdings_str = "(æœŸæœ«æ— èµ„äº§æ•°æ®)"
        else:
            df_end_assets = df_assets_all[mask_end].copy()
            total_end_val = df_end_assets['amount'].sum()
            df_end_assets['ratio'] = df_end_assets['amount'] / total_end_val
            key_assets = df_end_assets[df_end_assets['ratio'] > 0.005].sort_values('amount', ascending=False)
            
            # Markdown è¡¨å¤´
            lines = [f"å½“å‰æ€»èµ„äº§: **{total_end_val:,.2f}**\n"]
            lines.append("| èµ„äº§åç§° | å¸ç§ | é‡‘é¢ | å æ¯” | çŠ¶æ€ |")
            lines.append("|---|---|---|---|---|")
            
            for _, row in key_assets.iterrows():
                curr = row['currency'] if row['currency'] != 'CNY' else ""
                profit_txt = f"æµ®ç›ˆ {row['profit']:,.0f}" if row['profit'] > 0 else f"æµ®äº {row['profit']:,.0f}"
                lines.append(f"| **{row['name']}** | {curr} | {row['amount']:,.0f} | {row['ratio']*100:.2f}% | {profit_txt} |")
            
            top_holdings_str = "\n".join(lines)

        # --- 5. ç»„è£… Prompt (Prompt Engineering) ---
        prompt_content = f"""
# Role / è§’è‰²è®¾å®š
**ä½ æ˜¯ä¸€ä½æ‹¥æœ‰ 20 å¹´ç»éªŒçš„ä¸“ä¸šåŸºé‡‘æŠ•èµ„é¡¾é—® (CIO çº§åˆ«)ã€‚**
ä½ çš„ä¸“é•¿æ˜¯åŸºäºè¯¦å®çš„æ•°æ®ï¼Œå¯¹ä¸ªäººæŠ•èµ„è€…çš„æŠ•èµ„ç»„åˆè¿›è¡Œ**å½’å› åˆ†æ**ã€**é£é™©è¯„ä¼°**å’Œ**ç­–ç•¥å»ºè®®**ã€‚
ä½ å³å…³æ³¨å®è§‚å‘¨æœŸçš„å½±å“ï¼Œä¹Ÿå…³æ³¨å¾®è§‚æŒä»“çš„ç»“æ„å¥åº·åº¦ã€‚ä½ çš„åˆ†æé£æ ¼å®¢è§‚ã€ç†æ€§ï¼Œä¸”å–„äºå‘ç°æ•°æ®èƒŒåçš„éšæ‚£æˆ–æœºä¼šã€‚

# Context / åˆ†æèƒŒæ™¯
- **åˆ†æå‘¨æœŸ**: {start_date_str} è‡³ {end_date_str}
- **ç»Ÿè®¡ç»´åº¦**: {dimension_group}

# Data Section / æŠ•èµ„ç»„åˆæ•°æ®

## 1. æ¯æ—¥å‡€å€¼ä¸æ”¶ç›Šè¶‹åŠ¿ (Daily Trend)
ä¸ªäººæŠ•èµ„è€…çš„å…¨éƒ¨èµ„äº§å·²ç»å‡€å€¼åŒ–
{markdown_trend_str}

## 2. ç»“æ„å˜åŒ–å¯¹æ¯” (Structure Change)

*ç»´åº¦: {dimension_group} | å¯¹æ¯”: æœŸåˆ vs æœŸæœ«*
{structure_str}

## 3. æœŸæœ«æ ¸å¿ƒæŒä»“ (Key Holdings > 0.5%)

{top_holdings_str}

---

# Action Required / ä½ çš„ä»»åŠ¡

è¯·åŸºäºä¸Šè¿°æ•°æ®ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä»½ä¸“ä¸šçš„**ã€ŠæŠ•èµ„ç»„åˆå¤ç›˜æŠ¥å‘Šã€‹**ã€‚è¯·åŒ…å«ä»¥ä¸‹ç« èŠ‚ï¼š

### ç¬¬ä¸€éƒ¨åˆ†ï¼šå‘¨æœŸè¡¨ç°ç»¼è¿°

1. **æ”¶ç›Šå½’å› **ï¼šç»“åˆ Daily Trend æ•°æ®ï¼Œåˆ†æè¿™æ®µæ—¶é—´å‡€å€¼æ³¢åŠ¨çš„ä¸»è¦åŸå› ã€‚æ˜¯åœ¨å“ªå‡ å¤©å‘ç”Ÿäº†å¤§å¹…å›æ’¤æˆ–ä¸Šæ¶¨ï¼Ÿè¿™å¯èƒ½ä¸å½“æ—¶çš„ä»€ä¹ˆå¸‚åœºå¤§äº‹ä»¶æœ‰å…³ï¼Ÿï¼ˆè¯·ç»“åˆä½ çš„äº’è”ç½‘çŸ¥è¯†æ£€ç´¢è¯¥æ—¶é—´æ®µçš„å¸‚åœºæ–°é—»ï¼‰
2. **é£é™©æŒ‡æ ‡**ï¼šåŸºäºå‡€å€¼æ•°æ®ï¼Œä¼°ç®—è¿™æ®µæ—¶é—´çš„æœ€å¤§å›æ’¤ (Max Drawdown) å’Œæ³¢åŠ¨æƒ…å†µã€‚

### ç¬¬äºŒéƒ¨åˆ†ï¼šç»“æ„ä¸ä»“ä½åˆ†æ

1. **è°ƒä»“è¯„ä»·**ï¼šåŸºäº Structure Change è¡¨æ ¼ï¼Œåˆ†ææˆ‘åœ¨è¿™æ®µæ—¶é—´çš„ä¸»è¦èµ„é‡‘æµå‘ã€‚æˆ‘åŠ ä»“äº†ä»€ä¹ˆï¼Ÿå‡ä»“äº†ä»€ä¹ˆï¼Ÿè¿™ç§ç»“æ„è°ƒæ•´æ˜¯å¦è®©ç»„åˆå˜å¾—æ›´æŠ—è·Œæˆ–æ›´æ¿€è¿›ï¼Ÿ
2. **æŒä»“é›†ä¸­åº¦**ï¼šåŸºäº Key Holdings åˆ—è¡¨ï¼Œç‚¹è¯„æˆ‘çš„æŒä»“é›†ä¸­åº¦é£é™©ã€‚æ˜¯å¦å­˜åœ¨å•ä¸€èµ„äº§å æ¯”è¿‡é«˜çš„é—®é¢˜ï¼Ÿ

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šæœªæ¥å»ºè®®

1. åŸºäºå½“å‰çš„å®è§‚ç¯å¢ƒå’Œæˆ‘çš„æŒä»“ç»“æ„ï¼Œç»™å‡º 1-3 æ¡å…·ä½“çš„è°ƒæ•´å»ºè®®ï¼ˆå¦‚ï¼šæ˜¯å¦éœ€è¦å¢åŠ å€ºåˆ¸å¯¹å†²ï¼Ÿæ˜¯å¦éœ€è¦æ­¢ç›ˆæŸç±»èµ„äº§ï¼Ÿï¼‰ã€‚

"""
        # --- 6. å‘é€é‚®ä»¶ ---
        msg = MIMEMultipart()
        msg['Subject'] = f'ğŸ¤– AI æ·±åº¦æŠ•é¡¾ Prompt ({start_date_str} ~ {end_date_str})'
        msg['From'] = settings['email_user']
        msg['To'] = settings['email_to'] if settings['email_to'] else settings['email_user']
        
        body = "è¿™æ˜¯ä¸ºæ‚¨ç”Ÿæˆçš„ AI æŠ•é¡¾æç¤ºè¯ï¼ŒåŒ…å«äº†æ¯æ—¥å‡€å€¼æ•°æ®å’Œè¯¦ç»†æŒä»“ç»“æ„ã€‚\nè¯·å°†ä¸‹æ–¹å†…å®¹å®Œæ•´å¤åˆ¶ç»™ AI æ¨¡å‹ã€‚\n\n" + prompt_content
        msg.attach(MIMEText(body, 'plain'))

        server = smtplib.SMTP_SSL(settings['email_host'], settings['email_port'])
        server.login(settings['email_user'], settings['email_password'])
        server.send_message(msg)
        server.quit()
        
        return True, "Prompt å·²å‘é€è‡³é‚®ç®±ï¼",prompt_content

    except Exception as e:
        import traceback
        traceback.print_exc()
        return False, f"ç”Ÿæˆå¤±è´¥: {str(e)}"
    finally:
        conn.close()
 
# ==========================================
# 8. ç”¨æˆ·å…¬å‘Šæ¿ (æ–°å¢)
# ==========================================

def get_user_notice(user_id):
    """è·å–ç”¨æˆ·çš„ä¸ªäººå…¬å‘Š"""
    conn = get_db_connection()
    try:
        row = conn.execute('SELECT personal_notice FROM users WHERE user_id = ?', (user_id,)).fetchone()
        return row['personal_notice'] if row and row['personal_notice'] else ""
    except Exception:
        return ""
    finally:
        conn.close()

def update_user_notice(user_id, new_notice):
    """æ›´æ–°ç”¨æˆ·çš„ä¸ªäººå…¬å‘Š"""
    conn = get_db_connection()
    try:
        conn.execute('UPDATE users SET personal_notice = ? WHERE user_id = ?', (new_notice, user_id))
        conn.commit()
        return True
    except Exception as e:
        return False
    finally:
        conn.close()
                
def send_email_backup(filepath, settings):
    """å‘é€æ•°æ®åº“å¤‡ä»½é‚®ä»¶"""
    try:
        msg = MIMEMultipart()
        msg['Subject'] = f'å¤‡ä»½ {datetime.now().strftime("%Y-%m-%d")}'
        msg['From'] = settings['email_user']
        msg['To'] = settings['email_to'] or settings['email_user']
        
        with open(filepath, "rb") as f:
            part = MIMEApplication(f.read(), Name=os.path.basename(filepath))
            part['Content-Disposition'] = f'attachment; filename="{os.path.basename(filepath)}"'
            msg.attach(part)

        server = smtplib.SMTP_SSL(settings['email_host'], settings['email_port'])
        server.login(settings['email_user'], settings['email_password'])
        server.send_message(msg)
        try: server.quit() 
        except: pass
        return True, "å‘é€æˆåŠŸ"
    except Exception as e:
        return False, str(e)

def perform_backup(manual=False):
    """æ‰§è¡Œå¤‡ä»½"""
    conn = get_db_connection()
    settings = conn.execute('SELECT * FROM system_settings WHERE id = 1').fetchone()
    
    backup_dir = "backups"
    if not os.path.exists(backup_dir): os.makedirs(backup_dir)
    filename = f"asset_tracker_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
    path = os.path.join(backup_dir, filename)
    
    try:
        shutil.copy2(DB_FILE, path)
        status = "æœ¬åœ°æˆåŠŸ"
        if settings['email_host']:
            ok, msg = send_email_backup(path, settings)
            status += f" | é‚®ä»¶: {msg}"
        
        conn.execute('UPDATE system_settings SET last_backup_at = ? WHERE id = 1', 
                    (datetime.now().strftime('%Y-%m-%d %H:%M:%S'),))
        conn.commit()
        return True, status
    except Exception as e:
        return False, str(e)
    finally:
        conn.close()

def auto_backup_check():
    """è‡ªåŠ¨å¤‡ä»½æ£€æŸ¥ (å¯åœ¨ app.py å…¥å£è°ƒç”¨)"""
    conn = get_db_connection()
    try:
        row = conn.execute('SELECT backup_frequency, last_backup_at FROM system_settings WHERE id = 1').fetchone()
        if not row or row['backup_frequency'] == 'å…³é—­': return

        last = datetime.strptime(row['last_backup_at'], '%Y-%m-%d %H:%M:%S') if row['last_backup_at'] else datetime.min
        delta = (datetime.now() - last).days
        freq_map = {'æ¯å¤©':1, 'æ¯å‘¨':7, 'æ¯æœˆ':30}
        
        if delta >= freq_map.get(row['backup_frequency'], 999):
            st.toast("æ­£åœ¨è‡ªåŠ¨å¤‡ä»½...", icon="â³")
            perform_backup()
    except Exception:
        pass
    finally:
        conn.close()